<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>CS 180 Project 3: (Auto)Stitching and Photo Mosaics</title>
  <link rel="stylesheet" href="../style.css" />
  <!-- Optional: Google Fonts for Inter -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
  <link href="https://cdn.jsdelivr.net/npm/prismjs/themes/prism-tomorrow.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/prismjs/prism.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs/components/prism-python.min.js"></script>
 <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</head>
<body>

  <!-- ====== Navbar ====== -->
  <nav class="container">
    <ul>
      <li><a href="../index.html">Return to Home</a></li>
      <!-- <li><a href="#projects">Projects</a></li>
      <li><a href="#contact">Contact</a></li> -->
    </ul>
  </nav>

  <!-- ====== Hero Section ====== -->
  <header>
    <div>
      <h1>CS 180 Project 3: (Auto)Stitching and Photo Mosaics</h1>
      <p>In this project, we use homographic transformations and image blending techniques to stitch images into photo mosaics (i.e. panoramas).</p>
      <!--<a href="#projects" class="btn">See My Work</a> -->
    </div>
  </header>

  <!-- ====== Assignment Parts ====== -->
  <section class="container">
    <h2>Part A</h2>
    <section class="container">
      <h3>Part A.1: Shoot the Pictures</h2>
      <p>In order to create image mosaics, we first need the images to stitch together! Here are the photos I captured, including the nighttime view outside of my window, tiled mosaic next to the Social Sciences Building, and the Berkeley Rose Garden:</p>
      <p style="text-align: center;"><img src="./assets/windowview3.jpg" width="30%"> <img src="./assets/windowview2.jpg" width="30%">  <img src="./assets/windowview1.jpg" width="30%"></p>
      <p style="text-align: center;"><img src="./assets/tunnel_3.jpg" width="30%"> <img src="./assets/tunnel_2.jpg" width="30%">  <img src="./assets/tunnel_1.jpg" width="30%"></p>
      <p style="text-align: center;"><img src="./assets/Rosegarden1_resized.jpg" width="22.5%"> <img src="./assets/Rosegarden2_resized.jpg" width="22.5%">  <img src="./assets/Rosegarden3_resized.jpg" width="22.5%"> <img src="./assets/Rosegarden4_resized.jpg" width="22.5%"></p>
    </section>
     <section class="container">
      <h3>Part A.2: Recover Homographies</h2>
      <p>Next, we need to be able to compute the relative homographies between two images we want to align. To begin, we must register a sufficient number of corresponding points between both images (in this case we use 8). This is currently done by hand, but will be performed programmatically in Part 2 of the project. Here are the selected correspondences for one of the image pairs:</p>
      <p style="text-align: center;"><img src="./assets/points1.jpg" width="45%"> <img src="./assets/points2.jpg" width="45%"></p>
      <br>
      <p>With these point values, the matrix-vector system of equations is given by \( Ax = b \), where for a set of initial points \(x_i, y_i\) and their corresponding target points \(u_i, v_i\):</p>
      <p style="text-align: center;">\( 
        A = \begin{bmatrix} x_1 & y_1 & 1 & 0 & 0 & 0 & -x_1u_1 & -y_1u_1 \\ 
        0 & 0 & 0 & x_1 & y_1 & 1 & -x_1v_1 & -y_1v_1 \\ 
        x_2 & y_2 & 1 & 0 & 0 & 0 & -x_2u_2 & -y_2u_2 \\ 
        0 & 0 & 0 & x_2 & y_2 & 1 & -x_2v_2 & -y_2v_2 \\ 
        x_3 & y_3 & 1 & 0 & 0 & 0 & -x_3u_3 & -y_3u_3 \\ 
        0 & 0 & 0 & x_3 & y_3 & 1 & -x_3v_3 & -y_3v_3 \\
        \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots
   \end{bmatrix}, x = \begin{bmatrix} h_1 \\ h_2 \\ h_3 \\ h_4 \\ h_5 \\ h_6 \\ h_7 \\ h_8 \\ \end{bmatrix}, b = \begin{bmatrix}
u_1 \\
v_1 \\
u_2 \\
v_2 \\
u_3 \\
v_3 \\
\vdots
\end{bmatrix}\) </p>
      <p>Thus, for our particular image example, the system of equations is given by:</p>
      <p style="text-align: center;">\( A = \begin{bmatrix} 335.896104 & 122.746753 & 1 & 0 & 0 & 0 & -33787.0039 & -12346.8090 \\ 
        0 & 0 & 0 & 335.896104 & 122.746753 & 1 & -32014.8248 & -11699.2003 \\ 
        577.535714 & 274.694805 & 1 & 0 & 0 & 0 & -214102.615 & -101834.180 \\
        0 & 0 & 0 &  577.535714 & 274.694805 & 1 & -157427.235 & -74877.5229 \\
        858.217532 & 125.912338 & 1 & 0 & 0 & 0 & -544552.956 & -79893.4223 \\
        0 & 0 & 0 & 858.217532 & 125.912338 & 1 & -125266.323 & -18378.2956 \\
        744.256494 & 514.224026 & 1 & 0 & 0 & 0 & -399206.618 & -275821.086 \\
        0 & 0 & 0 & 744.256494 & 514.224026 & 1 & -377217.221 & -260628.103 \\
        733.704545 & 542.714286 & 1 & 0 & 0 & 0 & -385804.719 & -285376.087 \\
        0 & 0 & 0 & 733.704545 & 542.714286 & 1 & -391998.329 & -289957.442 \\
        596.529221 & 496.285714 & 1 & 0 & 0 & 0 & -234991.841 & -195502.734 \\
        0 & 0 & 0 & 596.529221 & 496.285714 & 1 & -297307.839 & -247346.866 \\
        631.350649 & 649.288961 & 1 & 0 & 0 & 0 & -271359.839 & -279069.876 \\
        0 & 0 & 0 & 631.350649 & 649.288961 & 1 & -409262.809 & -420891.029 \\
        570.149351 & 521.610390 & 1 & 0 & 0 & 0 & -207754.649 & -190067.713 \\
        0 & 0 & 0 & 570.149351 & 521.610390 & 1 & -299200.681 & -273728.600
   \end{bmatrix}, b = \begin{bmatrix}
100.58766234 \\
95.31168831 \\
370.71753247 \\
272.58441558 \\
634.51623377 \\
145.96103896 \\
536.38311688 \\
506.83766234 \\
525.83116883 \\
534.27272727 \\
393.93181818 \\
498.39610390 \\
429.80844156 \\
648.23376623 \\
364.38636364 \\
524.77597403
\end{bmatrix}\) </p>
      <br>
      <p>Once the system is solved using Least Squares, the resulting homographic transformation matrix between the two images is given by:</p>
      <p style="text-align: center;">\( H = \begin{bmatrix}
h_1 & h_2 & h_3 \\
h_4 & h_5 & h_6 \\
h_7 & h_8 & 1
\end{bmatrix} = \begin{bmatrix}
1.47139008 & -0.00504424883 & -378.805692 \\
0.169018117 & 1.26679848 & -101.769562 \\
0.000461900408 & -0.0000242372288 & 1
\end{bmatrix} \)</p>
    </section>
     <section class="container">
      <h3>Part A.3: Warp the Images</h2>
      <p>Now that we are able to derive the homographic transform between two images, we need to be able to actually apply it to warp our images. In order to verify our warping implementation, we will attempt to perform rectification on a couple of images. In the images below, I present the original image (left), the image rectified using nearest neighbor interpolation (center), and the image rectified using bilinear interpolation (right):</p>
      <p style="text-align: center;"><img src="./assets/bed.jpg" width="30%"> <img src="./assets/rect_bed_nn.jpg" width="30%">  <img src="./assets/rectified_bed.jpg" width="30%"></p>
      <p style="text-align: center;"><img src="./assets/campanile_small.jpg" width="30%"> <img src="./assets/rect_nile_nn.jpg" width="30%">  <img src="./assets/rect_nile.jpg" width="30%"></p>
      <p>The original image of the board above my bed was initially much more twisted than the image of the campanile, resulting in more dramatic rectification results. However, the scale of the image was very large and thus the nearest neighbor and bilinear interpolation methods are practically indistinguishable from one another. On the much smaller campanile images, however, we can observe carefully that some small features (e.g. the windows) appear more jagged and pixilated in nature on the nearest neighbor interpolation than the bilinear one. This reflects the fact that bilinear attempts to smoothly transition between pixel values while nearest neighbor simply discretely copies one value or another, leading to sharper changes.</p>
      </section>
    <section class="container">
      <h3>Part A.4: Blend the Images into a Mosaic</h2>
      <p>Now that we're able to warp the images, it is finally time to put them together into a mosaic! Once the images have been warped appropriately (warping one image onto the other image using the methods we've developed in the previous parts), we need to correctly mask which parts of each image to use. To do this, we can create alpha masks that fall off linearly from the centers of each image:
      <p style="text-align: center;"><img src="./assets/grad1_mask.jpg" width="45%"> <img src="./assets/grad2_mask.jpg" width="45%"></p>
      <p>Next, we can logically compare the values of these masks to derive which image is more applicable for each pixel. This comparison results in the following binary mask:</p>
      <p style="text-align: center;"><img src="./assets/my_mask.jpg" width="60%"></p>
      <p>Applying this binary mask naively to our transformed images can yield a reasonable result, but the border between images can still be apparent. As a result, we can use the Laplacian pyramid smoothing technique from last project to further blend the edges of the images together. The left image is an example with raw binary masking, while the right presents the same example with Laplacian smoothing:</p>
      <p style="text-align: center;"><img src="./assets/garden_blendless.jpg" width="45%"> <img src="./assets/Rosegarden12.jpg" width="45%"></p>
      <br>
      <p>Now that our blending pipeline is established, we can apply it iteratively to produce more mosaics. Each of the following mosaics were produced by sequentially pairwise blending images (e.g. blending 1 and 2, then blending the result with 3):</p>
      <p style="text-align: center;"><img src="./assets/windowview3.jpg" width="30%"> <img src="./assets/windowview2.jpg" width="30%">  <img src="./assets/windowview1.jpg" width="30%"></p>
      <p style="text-align: center;"><img src="./assets/full_windowview.jpg" width="70%"></p>
      <p style="text-align: center;"><img src="./assets/tunnel_3.jpg" width="30%"> <img src="./assets/tunnel_2.jpg" width="30%">  <img src="./assets/tunnel_1.jpg" width="30%"></p>
      <p style="text-align: center;"><img src="./assets/full_tunnel.jpg" width="70%"></p>
      <p style="text-align: center;"><img src="./assets/Rosegarden1_resized.jpg" width="22.5%"> <img src="./assets/Rosegarden2_resized.jpg" width="22.5%">  <img src="./assets/Rosegarden3_resized.jpg" width="22.5%"> <img src="./assets/Rosegarden4_resized.jpg" width="22.5%"></p>
      <p style="text-align: center;"><img src="./assets/Rosegarden3412.jpg" width="70%"></p>
      </section>
    </section>

<section class="container">
    <h2>Part B</h2>
    <section class="container">
      <h3>Part B.1: Harris Corner Detection</h2>
      <p>In part B of this project, we need to generate our correspondences programmatically. To do this, we can select point candidates using Harris corner detection, which should give us 'corners', or points that are easily distinguishable in the image. The result of running this algorithm on our nighttime image is as follows:</p>
      <p style="text-align: center;"><img src="./assets/b/harris_points_1_reg.jpg" width="45%"> <img src="./assets/b/harris_points_2_reg.jpg" width="45%"></p>
      <p>While most of the points selected probably are indeed 'corners', we can observe that the seen is more dense with selections than we really need. Ideally, we would like our selected points to be spatially distinct so that correspondences don't get confused. As a result, we can employ Adaptive Non-Maximal Suppression (ANMS), which only preserves the best 'corner' within a given radius. The resulting selections with this filtering can be seen as follows:</p>
      <p style="text-align: center;"><img src="./assets/b/harris_points_1_anms.jpg" width="45%"> <img src="./assets/b/harris_points_2_anms.jpg" width="45%"></p>
      <p>We can observe that we still keep a reasonable number of selected points, but now they are more spatially separated as we desired.</p>
    </section>
     <section class="container">
      <h3>B.2: Feature Descriptor Extraction</h2>
      <p>Now that we've selected our points of interest, we need a way to qualitatively describe them so that they can be properly compared with neighbors from the other image view. Thus, rather than just taking individual pixel values, we would like to sample small patches around the points in question to get a more holistic view of each point in question. In order to account for potential lighting differences, we must make sure to normalize our patches. Because we don't want the descriptors to be biased by the effects of noise, we will also smooth an initial 40 x 40 sampled patch into a smaller 8 x 8 representation, which should demonstrate the more fundamental low-frequency characteristics that identify a particular 'corner' in question. Here are some examples of generated feature patches:</p>
      <p style="text-align: center;"><img src="./assets/b/image_patch1_0.jpg" width="17%"> <img src="./assets/b/image_patch1_1.jpg" width="17%"> <img src="./assets/b/image_patch1_2.jpg" width="17%"> <img src="./assets/b/image_patch1_3.jpg" width="17%"> <img src="./assets/b/image_patch1_4.jpg" width="17%"></p>
      <p style="text-align: center;"><img src="./assets/b/image_patch1_5.jpg" width="17%"> <img src="./assets/b/image_patch1_6.jpg" width="17%"> <img src="./assets/b/image_patch1_7.jpg" width="17%"> <img src="./assets/b/image_patch1_8.jpg" width="17%"> <img src="./assets/b/image_patch1_9.jpg" width="17%"></p>
      <br>
    </section>
     <section class="container">
      <h3>Part B.3: Feature Matching</h2>
      <p>With our feature patches generated, we can now compare them to find correspondences! To do this, we can compare the Euclidean distances between image patches in 64-dimensional space and use Lowe thresholding to select only pairs that are similar to one another and no other patches. Doing so gives us a subset of the original 'corners' that have correspondences, which can be visualized between our images as follows:</p>
      <p style="text-align: center;"><img src="./assets/b/correspondences.jpg" width="90%"></p>
      </section>
    <section class="container">
      <h3>Part B.4: RANSAC for Robust Homography</h2>
      <p>Given all of our candidate correspondences from feature matching, we now want to figure out an optimal transformation between our two images to actually generate our mosaic. To do this, we will employ RANSAC (Random Sample Consensus), which involves selecting subsets of correspondences, computing a hypothetical transform, comparing that transform to all of the other correspondences, and counting that number that 'agree' (the transformation error is small). After trying a number of samples, we can be assured that the sample with the most "inliers" (correspondences that have low error) best represents 'true' correspondences that are present between the two images (all of the 'inliers'). This set of correspondences can then be used to recompute a final image transformation to use for our mosaic.</p>
      <br>
      <p>With our transformation automatically derived, we can use the same image transformation and blending techniques from Part A to auto-generate mosaics from scratch. The resulting mosaics on our captured images are as follows (once more produced by sequential pairwise blending as described above): </p>
      <p style="text-align: center;"><img src="./assets/windowview3.jpg" width="30%"> <img src="./assets/windowview2.jpg" width="30%">  <img src="./assets/windowview1.jpg" width="30%"></p>
      <p style="text-align: center;"><img src="./assets/b/windowview_blended.jpg" width="70%"></p>
      <p style="text-align: center;"><img src="./assets/tunnel_3.jpg" width="30%"> <img src="./assets/tunnel_2.jpg" width="30%">  <img src="./assets/tunnel_1.jpg" width="30%"></p>
      <p style="text-align: center;"><img src="./assets/b/tunnel_blended.jpg" width="70%"></p>
      <p style="text-align: center;"><img src="./assets/Rosegarden1_resized.jpg" width="22.5%"> <img src="./assets/Rosegarden2_resized.jpg" width="22.5%">  <img src="./assets/Rosegarden3_resized.jpg" width="22.5%"> <img src="./assets/Rosegarden4_resized.jpg" width="22.5%"></p>
      <p style="text-align: center;"><img src="./assets/b/Rosegarden_blended.jpg" width="70%"></p>
      <p>For reference, we can compare these results (left) with the blending results using manual correspondences (right):</p>
      <p style="text-align: center;"><img src="./assets/b/windowview_blended.jpg" width="45%"> <img src="./assets/full_windowview.jpg" width="45%"></p>
      <p style="text-align: center;"><img src="./assets/b/tunnel_blended.jpg" width="45%"> <img src="./assets/full_tunnel.jpg" width="45%"></p>
      <p style="text-align: center;"><img src="./assets/b/Rosegarden_blended.jpg" width="45%"> <img src="./assets/Rosegarden3412.jpg" width="45%"></p>

      <p>For the most part, the blending results of the manual and automatic methods are rather similar, since both methods of correspondence selection ended up being decent in quality. That said, the result from automatic correspondences appears to have eliminated some small discrepancies that were present in the manual version, as well as tweaking some of the homographies. The edges between images are slightly different, as a result of these modifications to transformation. </p>
      </section>
    </section>

  <!-- <section class="container">
    <h2>Part 2: Architectural Perspective Compression</h2>
    <p>
      Zooming in on the cherry trees on the west side of campus.
    </p>
    <p style="text-align: center;"><img src="./media/path_normal.jpeg" width="45%">
    <img src="./media/path_zoomed.jpeg" width="45%"></p>
  </section>
  <section class="container">
    <h2>Part 3: The Dolly Zoom</h2>
    <p>
      Recreating "Vertigo shot" effect, with a fossilized co-star.
    </p>
    <p style="text-align: center;"><img src="./media/dolly_zoom.gif" width="70%">
    </p>
  </section>

  <section id="comparisons" class="container">
  <h2>Small Images</h2>
  <p>Please note that shift amounts are denoted as (x, y)</p> -->
  
 

  <!-- ====== Footer ====== -->
<!--   <footer>
    <p>© 2025 Jaimyn Drake. Built with ❤️ and lots of robotics coffee.</p>
  </footer> -->

</body>
</html>
<!-- ====== Boilerplate provided by ChatGPT ====== -->
